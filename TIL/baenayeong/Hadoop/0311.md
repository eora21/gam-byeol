하둡 파일로 데이터를 어떻게 읽고 쓰는지는 알겠는데 이걸 **Spring에서 어떻게 프론트에서 넘어온 데이터를 읽고 쓸지** 감이 오지 않아서 조사해보았다.

# ‘하둡 데이터 저장’ 키워드로 검색

## 아파치 하이브

- 하둡에 저장된 데이터를 쉽게 처리할 수 있는 데이터웨어하우스 패키지이다.
- **맵리듀스 코드를 SQL과 유사한 쿼리로 처리**할 수 있도록 하고, 파일 시스템에 저장된 데이터에 catalog와 metastore를 제공해서 **논리적 데이터베이스, 테이블을 제공**해서 데이터를 구조화할 수 있게 한다.
- SQL 레벨의 ETL 처리 도구로 활용이 가능하다.
- 작성된 쿼리를 내부적으로 **맵리듀스 형태로 변환**한다.

### HCatalog

Pig, MapReduce, Spark에서 하이브의 데이터 파일에 접근할 수 있도록 도와주는 추상 계층을 제공한다.

```java
% spark-shell --jars /usr/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core-1.0.0-amzn-3.jar
scala> val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc); 
scala> val df = hiveContext.sql("SELECT * FROM impressions") 
scala> df.show()
```

### WebHCat

HCatalog의 기능을 REST API로 제공한다. 기본적으로는 50111 포트를 사용한다.

```java
# WebHCat 버전 확인 
curl -s 'http://localhost:50111/templeton/v1/version'

# 명령 실행 
curl -s -d execute="show tables" \
-d statusdir="pokes.output" \
       'http://localhost:50111/templeton/v1/hive?user.name=root'
```

### WebHDFS

하둡 파일 시스템의 접근을 Web REST API 방식으로 지원한다.

→ 그렇다면 백엔드에서 Rest API 호출해주는 방식으로 데이터를 접근하게 되는 건가?

## 참고 링크

[https://wikidocs.net/23683](https://wikidocs.net/23683)

WebHDFS 

[https://hadoop.apache.org/docs/r1.0.4/webhdfs.html](https://hadoop.apache.org/docs/r1.0.4/webhdfs.html) 

[https://hevodata.com/learn/webhdfs/](https://hevodata.com/learn/webhdfs/)

# ‘Spring Hadoop’ 키워드로 검색

하이브를 검색할수록 딱히 자바에서 어떻게 작성을 해야 정보를 저장할지는 나오지 않았다. 그것보다는 이미 저장된 하둡의 파일을 어떻게 쿼리문으로 읽어오는지에 대한 정보랑 REST API 사용법들만 나왔다... 그래서 Sprint Hadoop으로 서치하기 시작했다.

## 하둡에서 Multipart 파일 저장하기

Spring for Apache Hadoop 이라는 라이브러리가 있다. 이걸 가져다가 사용하면 된다고 한다. 자바 소스 코드를 작성하기 전에 

1. 스프링 버전에 맞는 라이브러리 버전을 찾기
2. 하둡 네임노드의 설정을 Config 파일에 추가하기

두 가지 과정이 필요하다. 자세한 내용은 [https://devidea.tistory.com/61](https://devidea.tistory.com/61) 에 있다.

백엔드 코드로 하둡에 Multipart파일을 저장하는 코드는 아래와 같다. 

![image](https://user-images.githubusercontent.com/55578809/157881950-759c283c-844e-4be4-a692-b37ffa897add.png)


## Spring 프로젝트에 하둡 사용하기

초기 세팅: [https://kplog.tistory.com/244](https://kplog.tistory.com/244) 

# 🤔의문점

기존의 스프링 프로젝트에서 데이터베이스가 맡던 역할을 하둡이 맡는 거라고 생각했는데 알아보면 알아볼수록 백엔드 서버에서 하둡으로 바로 데이터를 보내는 게 아닌 것 같다. 스터디에서 카프카 발표를 들으니까 백엔드 서버에서 카프카로 데이터를 보내도록 하고, 카프카에서 하둡에 저장 요청을 보내는 식이 될 것 같은데, 이러면 테스트 환경에서는 오히려 시간이 더 들지 않을지 의문이 들었다. 아예 **RDB를 하나 두고 거기에 데이터를 저장하도록 한 다음 사용자 분석을 위한 데이터만 하둡에 저장하도록 해야 하는 걸까?** 내가 이해한 것이 맞는지 고민이 된다. 현업에서는 어떻게 사용하는지 궁금하다.